
"""
MovieLens ETL Pipeline avec Apache Spark

Ce script impl√©mente un pipeline ETL complet pour transformer les donn√©es brutes 
MovieLens en un dataset silver enrichi et structur√©.

Architecture ETL:
- EXTRACT: Chargement des CSV depuis HDFS
- TRANSFORM: Nettoyage, enrichissement et agr√©gation des donn√©es
- LOAD: Sauvegarde au format Parquet pour analyse

Auteur: Tanya TIBOUCHE - Fitahiany Mich√®le MBOHOAZY
Date: Janvier 2026
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, avg, year, regexp_extract, explode, split, 
    round as spark_round, desc, trim
)
from pyspark.sql.types import IntegerType, FloatType
import sys
import logging

# Configuration du logging pour suivre l'ex√©cution
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_spark_session(app_name="MovieLens-ETL"):
    """
    Cr√©e et configure une session Spark
    
    SparkSession est le point d'entr√©e pour utiliser Spark.
    Il configure:
    - La connexion au cluster Hadoop
    - Les ressources m√©moire et CPU
    - Les param√®tres d'optimisation
    
    Args:
        app_name: Nom de l'application Spark
        
    Returns:
        SparkSession configur√©e
    """
    logger.info(f"Cr√©ation de la session Spark: {app_name}")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    # D√©finir le niveau de log pour r√©duire la verbosit√©
    spark.sparkContext.setLogLevel("WARN")
    
    logger.info(f"Session Spark cr√©√©e - Version: {spark.version}")
    return spark


def extract_data(spark, hdfs_path="/user/hadoop/movielens/"):
    """
    EXTRACT: Charge les donn√©es brutes depuis HDFS
    
    Cette fonction lit les fichiers CSV stock√©s dans HDFS et cr√©e des DataFrames.
    Les DataFrames sont des collections distribu√©es de donn√©es organis√©es en colonnes,
    similaires √† des tables SQL ou des DataFrames pandas mais distribu√©es.
    
    Args:
        spark: Session Spark active
        hdfs_path: Chemin HDFS o√π se trouvent les fichiers CSV
        
    Returns:
        dict: Dictionnaire contenant les DataFrames movies et ratings
    """
    logger.info("=" * 60)
    logger.info("PHASE EXTRACT: Chargement des donn√©es depuis HDFS")
    logger.info("=" * 60)
    
    try:
        # Chargement du fichier movies.csv
        # Options:
        # - header=True: La premi√®re ligne contient les noms de colonnes
        # - inferSchema=True: Spark d√©tecte automatiquement les types de donn√©es
        logger.info(f"Chargement de {hdfs_path}movies.csv")
        movies_df = spark.read.csv(
            f"{hdfs_path}movies.csv",
            header=True,
            inferSchema=True
        )
        
        logger.info(f"Chargement de {hdfs_path}ratings.csv")
        ratings_df = spark.read.csv(
            f"{hdfs_path}ratings.csv",
            header=True,
            inferSchema=True
        )
        
        # Affichage des informations sur les DataFrames
        logger.info(f"\nüìä Movies DataFrame:")
        logger.info(f"   - Nombre de lignes: {movies_df.count():,}")
        logger.info(f"   - Nombre de colonnes: {len(movies_df.columns)}")
        logger.info(f"   - Colonnes: {movies_df.columns}")
        movies_df.printSchema()
        
        logger.info(f"\nüìä Ratings DataFrame:")
        logger.info(f"   - Nombre de lignes: {ratings_df.count():,}")
        logger.info(f"   - Nombre de colonnes: {len(ratings_df.columns)}")
        logger.info(f"   - Colonnes: {ratings_df.columns}")
        ratings_df.printSchema()
        
        # Affichage des premi√®res lignes pour v√©rification
        logger.info("\nüîç Aper√ßu des donn√©es movies:")
        movies_df.show(5, truncate=False)
        
        logger.info("\nüîç Aper√ßu des donn√©es ratings:")
        ratings_df.show(5, truncate=False)
        
        return {
            "movies": movies_df,
            "ratings": ratings_df
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement des donn√©es: {str(e)}")
        sys.exit(1)


def transform_data(dataframes):
    """
    TRANSFORM: Nettoie, enrichit et agr√®ge les donn√©es
    
    Cette fonction applique plusieurs transformations:
    1. Extraction de l'ann√©e de sortie depuis le titre
    2. Explosion des genres (1 ligne par genre par film)
    3. Agr√©gation des ratings (moyenne et compte)
    4. Jointure des donn√©es movies et ratings
    
    Concepts Spark importants:
    - withColumn(): Ajoute ou modifie une colonne
    - regexp_extract(): Extraction par expression r√©guli√®re
    - explode(): Transforme un array en plusieurs lignes
    - groupBy() + agg(): Agr√©gation distribu√©e
    - join(): Jointure distribu√©e entre DataFrames
    
    Args:
        dataframes: Dict contenant les DataFrames movies et ratings
        
    Returns:
        DataFrame: Dataset silver enrichi et structur√©
    """
    logger.info("\n" + "=" * 60)
    logger.info("PHASE TRANSFORM: Transformation des donn√©es")
    logger.info("=" * 60)
    
    movies_df = dataframes["movies"]
    ratings_df = dataframes["ratings"]
    
    # -----------------------------------------------------------------------
    # √âtape 1: Extraction de l'ann√©e de sortie
    # -----------------------------------------------------------------------
    logger.info("\nüîß √âtape 1: Extraction de l'ann√©e depuis le titre")
    
    # Expression r√©guli√®re pour extraire l'ann√©e entre parenth√®ses
    # Exemple: "Toy Story (1995)" -> 1995
    movies_with_year = movies_df.withColumn(
        "year",
        regexp_extract(col("title"), r"\((\d{4})\)", 1).cast(IntegerType())
    )
    
    # Nettoyage du titre (enlever l'ann√©e)
    movies_with_year = movies_with_year.withColumn(
        "clean_title",
        regexp_extract(col("title"), r"^(.*?)\s*\(\d{4}\)", 1)
    )
    
    logger.info("‚úÖ Ann√©es extraites et titres nettoy√©s")
    movies_with_year.select("movieId", "title", "clean_title", "year").show(5, truncate=False)
    
    # -----------------------------------------------------------------------
    # √âtape 2: Explosion des genres
    # -----------------------------------------------------------------------
    logger.info("\nüîß √âtape 2: Explosion des genres (1 ligne par genre)")
    
    # Les genres sont s√©par√©s par '|' dans le CSV
    # Exemple: "Adventure|Animation|Children" devient 3 lignes s√©par√©es
    # explode() est une transformation qui "d√©roule" un array en plusieurs lignes
    movies_exploded = movies_with_year.withColumn(
        "genre",
        explode(split(col("genres"), "\\|"))
    )
    
    # Nettoyage des espaces et gestion du cas "(no genres listed)"
    movies_exploded = movies_exploded.withColumn(
        "genre",
        trim(col("genre"))
    ).filter(
        col("genre") != "(no genres listed)"
    )
    
    logger.info("‚úÖ Genres explos√©s")
    logger.info(f"   Nombre de lignes apr√®s explosion: {movies_exploded.count():,}")
    movies_exploded.select("movieId", "clean_title", "year", "genre").show(10)
    
    # -----------------------------------------------------------------------
    # √âtape 3: Agr√©gation des ratings
    # -----------------------------------------------------------------------
    logger.info("\nüîß √âtape 3: Calcul des statistiques de rating")
    
    # GroupBy distribue le calcul sur tous les workers du cluster
    # agg() permet de calculer plusieurs agr√©gations en une passe
    ratings_agg = ratings_df.groupBy("movieId").agg(
        count("rating").alias("num_ratings"),
        spark_round(avg("rating"), 2).alias("avg_rating")
    )
    
    logger.info("‚úÖ Agr√©gations calcul√©es")
    logger.info(f"   Nombre de films avec des ratings: {ratings_agg.count():,}")
    ratings_agg.orderBy(desc("num_ratings")).show(10)
    
    # -----------------------------------------------------------------------
    # √âtape 4: Jointure finale
    # -----------------------------------------------------------------------
    logger.info("\nüîß √âtape 4: Jointure movies + ratings")
    
    # Left join pour garder tous les films, m√™me sans ratings
    # La jointure est distribu√©e: Spark optimise automatiquement
    silver_dataset = movies_exploded.join(
        ratings_agg,
        on="movieId",
        how="left"
    )
    
    # Remplacement des valeurs nulles pour les films sans ratings
    silver_dataset = silver_dataset.fillna({
        "num_ratings": 0,
        "avg_rating": 0.0
    })
    
    # S√©lection et r√©organisation des colonnes finales
    silver_dataset = silver_dataset.select(
        col("movieId").alias("movie_id"),
        col("clean_title").alias("movie_name"),
        col("year").alias("year_of_release"),
        col("num_ratings").alias("number_of_ratings"),
        col("genre"),
        col("avg_rating").alias("rating_average")
    )
    
    logger.info("‚úÖ Jointure termin√©e")
    logger.info(f"\nüìä Dataset Silver final:")
    logger.info(f"   - Nombre total de lignes: {silver_dataset.count():,}")
    logger.info(f"   - Sch√©ma:")
    silver_dataset.printSchema()
    
    logger.info("\nüîç Aper√ßu du dataset final:")
    silver_dataset.orderBy(desc("number_of_ratings")).show(20, truncate=False)
    
    return silver_dataset


def load_data(silver_dataset, output_path="/user/hadoop/movielens/silver/"):
    """
    LOAD: Sauvegarde le dataset silver au format Parquet
    
    Parquet est un format de fichier columnar optimis√© pour:
    - La compression (fichiers plus petits)
    - Les requ√™tes analytiques (lecture rapide de colonnes sp√©cifiques)
    - La compatibilit√© avec tout l'√©cosyst√®me big data
    
    Le mode "overwrite" √©crase les donn√©es existantes.
    
    Args:
        silver_dataset: DataFrame √† sauvegarder
        output_path: Chemin HDFS de destination
    """
    logger.info("\n" + "=" * 60)
    logger.info("PHASE LOAD: Sauvegarde du dataset silver")
    logger.info("=" * 60)
    
    try:
        logger.info(f"üíæ Sauvegarde vers: {output_path}")
        logger.info("   Format: Parquet (columnar, compress√©)")
        
        # Repartition pour optimiser la taille des fichiers
        # coalesce(10) r√©duit le nombre de partitions √† 10
        silver_dataset.coalesce(10).write.parquet(
            output_path,
            mode="overwrite"
        )
        
        logger.info("‚úÖ Sauvegarde r√©ussie!")
        logger.info(f"   Les donn√©es sont disponibles dans HDFS: {output_path}")
        
        # Optionnel: sauvegarder aussi en CSV pour compatibilit√©
        csv_path = output_path.replace("/silver/", "/silver_csv/")
        logger.info(f"\nüíæ Sauvegarde suppl√©mentaire en CSV: {csv_path}")
        
        silver_dataset.coalesce(1).write.csv(
            csv_path,
            mode="overwrite",
            header=True
        )
        
        logger.info("‚úÖ CSV sauvegard√© √©galement")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la sauvegarde: {str(e)}")
        sys.exit(1)


def main():
    """
    Fonction principale orchestrant le pipeline ETL complet
    
    Workflow:
    1. Cr√©ation de la session Spark
    2. EXTRACT: Chargement des donn√©es brutes
    3. TRANSFORM: Transformations et enrichissements
    4. LOAD: Sauvegarde du dataset silver
    5. Nettoyage et fermeture
    """
    logger.info("\n" + "üé¨ " * 20)
    logger.info("D√âMARRAGE DU PIPELINE ETL MOVIELENS")
    logger.info("üé¨ " * 20 + "\n")
    
    spark = None
    
    try:
        # Cr√©ation de la session Spark
        spark = create_spark_session()
        
        # Ex√©cution du pipeline ETL
        dataframes = extract_data(spark)
        silver_dataset = transform_data(dataframes)
        load_data(silver_dataset)
        
        # R√©sum√© final
        logger.info("\n" + "üéâ " * 20)
        logger.info("PIPELINE ETL TERMIN√â AVEC SUCC√àS!")
        logger.info("üéâ " * 20)
        logger.info("\nüìã R√©sum√©:")
        logger.info(f"   ‚úì Donn√©es extraites de HDFS")
        logger.info(f"   ‚úì Transformations appliqu√©es")
        logger.info(f"   ‚úì Dataset silver sauvegard√© en Parquet et CSV")
        logger.info(f"\n‚û°Ô∏è  Prochaine √©tape: Ex√©cuter spark-data-analysis.py")
        
    except Exception as e:
        logger.error(f"\n‚ùå Erreur fatale dans le pipeline: {str(e)}")
        sys.exit(1)
        
    finally:
        # Toujours fermer la session Spark pour lib√©rer les ressources
        if spark:
            logger.info("\nüîí Fermeture de la session Spark...")
            spark.stop()
            logger.info("‚úÖ Session ferm√©e")


if __name__ == "__main__":
    main()